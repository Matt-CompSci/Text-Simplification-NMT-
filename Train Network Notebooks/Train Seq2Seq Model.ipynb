{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainFYP.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1_-uuyP1HnOnJDbsK5ZUMVDcb78Lk1C0Z","authorship_tag":"ABX9TyO7iwzRrgyf8bBjJ6aoq7Rp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2da4PUWr2Ca","executionInfo":{"status":"ok","timestamp":1618921156647,"user_tz":-60,"elapsed":14241,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"5c3f754a-82b5-4441-cd00-f576a3076d44"},"source":["import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","import os\n","\n","# try:\n","#     device_name = os.environ['COLAB_TPU_ADDR']\n","#     TPU_ADDRESS = 'grpc://' + device_name\n","#     print('Found TPU')\n","# except KeyError:\n","#     print('TPU not found')\n","\n","# tf.config.experimental_connect_to_cluster(TPU_ADDRESS)\n","# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","# tf.tpu.experimental.initialize_tpu_system(resolver)\n","# strategy = tf.distribute.experimental.TPUStrategy(resolver) \n","\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"\")\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices(\"TPU\"))\n","\n","strategy = tf.distribute.TPUStrategy(resolver)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.109.72.50:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.109.72.50:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n","INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYHbF4AONwky","executionInfo":{"status":"ok","timestamp":1618921178058,"user_tz":-60,"elapsed":10114,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"d86e0c2a-ac20-4531-f224-af03a21cba4d"},"source":["import numpy as np\n","# Import glove embeddings\n","embeddings_index = {}\n","embeddingFile = open(\"drive/MyDrive/glove.6B.100d.txt\", encoding=\"utf-8\")\n","for line in embeddingFile:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype=\"float32\")\n","    embeddings_index[word] = coefs\n","embeddingFile.close()\n","print(\"Number of words in glove embeddings = \", len(embeddings_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of words in glove embeddings =  400000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hzr38wOyxJFO","executionInfo":{"status":"ok","timestamp":1618921224891,"user_tz":-60,"elapsed":15702,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"d3c898a8-9094-4c4a-b651-5e7d1f66f81a"},"source":["import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","\n","def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","    return pad_sequences(x, maxlen=length, padding=\"post\")\n","\n","\n","def tokenize(sentences):\n","    tokenizer = Tokenizer(oov_token=\"\")\n","    tokenizer.fit_on_texts(sentences)\n","    return tokenizer.texts_to_sequences(sentences), tokenizer\n","\n","def load_data(path):\n","    \"\"\"\n","    Load dataset\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\") as f:\n","        data = f.read()\n"," \n","    return data.split('\\n')\n","\n","normal_sentences = load_data(\"drive/MyDrive/normal.training.txt\")\n","simple_sentences = load_data(\"drive/MyDrive/simple.training.txt\")\n","\n","sentence_amount = len(normal_sentences)\n","normal_sentences = normal_sentences[:sentence_amount]\n","simple_sentences = simple_sentences[:sentence_amount]\n"," \n","print(\"Normal Sentences Loaded: \", len(normal_sentences))\n","print(\"Simple Sentences Loaded: \", len(simple_sentences))\n"," \n","wikipedia_sentences = []\n","simple_wikipedia_sentences = []\n"," \n","for sentence in normal_sentences:\n","    #sentence = re.sub(\"[^\\\\w\\\\s]\", \"\", sentence)\n","    sentence_split = sentence.split(\" \")\n"," \n","    while '' in sentence_split:\n","        sentence_split.remove('')\n"," \n","    wikipedia_sentences.append(sentence_split)\n"," \n","for sentence in simple_sentences:\n","    #sentence = re.sub(\"[^\\\\w\\\\s]\", \"\", sentence)\n","    sentence_split = sentence.split(\" \")\n"," \n","    while '' in sentence_split:\n","        sentence_split.remove('')\n"," \n","    simple_wikipedia_sentences.append(sentence_split)\n"," \n"," \n","indexesToRemove = []\n"," \n","# Remove \"weird\" sentences\n","for idx, sentence in enumerate(wikipedia_sentences):\n","    if len(sentence) > 75 or len(simple_wikipedia_sentences[idx]) > 75:\n","        indexesToRemove.append(idx)\n"," \n","wikipedia_sentences = [v for i,v in enumerate(wikipedia_sentences) if i not in indexesToRemove]\n","simple_wikipedia_sentences = [v for i,v in enumerate(simple_wikipedia_sentences) if i not in indexesToRemove]\n"," \n"," \n","# Tokenize input/output sentences\n","input_sequences, input_tokenizer = tokenize(wikipedia_sentences)\n","print(\"First Input: \", input_sequences[0])\n","output_sequences, output_tokenizer = tokenize(simple_wikipedia_sentences)\n","print(\"First output: \", output_sequences[0])\n"," \n","# Get vocab size of simple and regular wikipedia\n","regular_vocab_size = len(input_tokenizer.word_index)\n","simple_vocab_size = len(output_tokenizer.word_index)\n"," \n","# Pad input/output sentences\n","input_sequences = pad(input_sequences)\n","output_sequences = pad(output_sequences)\n"," \n","# Sparse categorical_crossentropy function requires the labels to be in 3 dimensions\n","output_sequences = output_sequences.reshape(*output_sequences.shape, 1)\n"," \n","# Get maximum length of simple and regular wikipedia sentences\n","max_sentence_length = input_sequences.shape[1]\n","max_simple_sentence_length = output_sequences.shape[1]\n"," \n","# Add additional padding so they're the same length\n","if(input_sequences.shape[1] > output_sequences.shape[1]):\n","    output_sequences = pad(output_sequences, input_sequences.shape[1])\n","else:\n","    input_sequences = pad(input_sequences, output_sequences.shape[1])\n"," \n"," \n","print(\"Max Regular wikipedia sentence length:\", max_sentence_length)\n","print(\"Max Simple wikipedia sentence length:\", max_simple_sentence_length)\n","print(\"Regular wikipedia vocab size:\", regular_vocab_size)\n","print(\"Simple wikipedia vocab size:\", simple_vocab_size)\n"," \n","input_sequences = input_sequences.reshape((-1, output_sequences.shape[-2], 1))\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Normal Sentences Loaded:  123627\n","Simple Sentences Loaded:  123627\n","First Input:  [62, 555, 13627, 28, 54086, 6, 8, 27182, 157, 3, 2507, 354, 88, 12, 2, 7545, 95, 39345, 11207, 4]\n","First output:  [2576, 11, 2, 8634, 4245, 13, 2, 574, 839, 7098, 92, 2, 736, 1099, 170, 9, 2, 711, 35151, 3, 54, 58, 167, 4245, 8014, 575, 11, 3900, 6, 7, 35151, 155, 4, 2576, 82, 2, 9345, 3]\n","Max Regular wikipedia sentence length: 75\n","Max Simple wikipedia sentence length: 75\n","Regular wikipedia vocab size: 110223\n","Simple wikipedia vocab size: 100602\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gd9huOxhOCRA"},"source":["embedding_matrix = np.zeros((regular_vocab_size, 100))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MAWJaS2t_D8","executionInfo":{"status":"ok","timestamp":1618515099953,"user_tz":-60,"elapsed":40187,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"54285a16-a99d-41d6-a1fa-0025f863fd15"},"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers.schedules import ExponentialDecay\n","\n","early_stopping = EarlyStopping(monitor='loss', patience=3)\n","checkpoint = ModelCheckpoint(\"drive/MyDrive/tpuModel5.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='auto', period=10)\n","\n","lr_schedule = ExponentialDecay(\n","    1e-3,\n","    decay_steps = 1e5,\n","    decay_rate = 0.96,\n","    staircase=True\n",")\n","\n","with strategy.scope():\n","  network = Sequential()\n","  network.add(Embedding(regular_vocab_size +1, 100, input_length=input_sequences.shape[1]))\n","  network.add(Bidirectional(GRU(300, return_sequences=False)))\n","  network.add(RepeatVector(max_sentence_length))\n","  network.add(Bidirectional(GRU(300, return_sequences=True)))\n","  network.add(TimeDistributed(Dense(simple_vocab_size + 1, activation=\"softmax\")))\n","\n","  network = Sequential()\n","  network.add(Embedding(regular_vocab_size +1, 100, input_length=input_sequences.shape[1]))\n","  network.add(Dropout(0.4))\n","  network.add(Bidirectional(GRU(300, return_sequences=False)))\n","  network.add(RepeatVector(max_sentence_length))\n","  network.add(Bidirectional(GRU(300, return_sequences=True)))\n","  network.add(Dropout(0.4))\n","  network.add(TimeDistributed(Dense(simple_vocab_size + 1, activation=\"softmax\")))\n","  \n","  network.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(lr_schedule), metrics=[\"accuracy\"])\n","  network.fit(input_sequences, output_sequences, batch_size=512, epochs=300, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n","\n","#   network.load_weights(\"drive/MyDrive/tpuModel2.hdf5\")\n","#   network.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(1e-4), metrics=[\"accuracy\"])\n","#   network.fit(input_sequences, output_sequences, batch_size=1024, epochs=2000, validation_split=0.2, callbacks=[checkpoint])\n","\n","#   network.load_weights(\"drive/MyDrive/tpuModel2.hdf5\")\n","#   network.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(1e-5), metrics=[\"accuracy\"])\n","#   network.fit(input_sequences, output_sequences, batch_size=1024, epochs=2000, validation_split=0.2, callbacks=[checkpoint])\n","\n","#   network.load_weights(\"drive/MyDrive/tpuModel2.hdf5\")\n","#   network.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(1e-6), metrics=[\"accuracy\"])\n","#   network.fit(input_sequences, output_sequences, batch_size=1024, epochs=1000, validation_split=0.2, callbacks=[checkpoint])\n","\n","#   network.load_weights(\"drive/MyDrive/tpuModel2.hdf5\")\n","#   network.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(1e-7), metrics=[\"accuracy\"])\n","#   network.fit(input_sequences, output_sequences, batch_size=1024, epochs=1000, validation_split=0.2, callbacks=[checkpoint])\n","\n","\n","\n","  \n","\n","\n","# network.save_weights(\"drive/MyDrive/network2.h5\")\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STkO7WALI5xH","executionInfo":{"status":"ok","timestamp":1618515131236,"user_tz":-60,"elapsed":71465,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"a6af835a-8602-47a6-c37e-eb8e3ecb0dc0"},"source":["def outputToText(logits, tokenizer):\n","  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n","  index_to_words[0] = ''\n","  predictions = [index_to_words[prediction] for prediction in np.argmax(logits, 1)]\n","\n","  noDuplicatePredictions = []\n","\n","  for idx, prediction in enumerate(predictions):\n","    if idx == 0:\n","      noDuplicatePredictions.append(prediction)\n","    else:\n","      if predictions[idx - 1] != prediction:\n","        noDuplicatePredictions.append(prediction)\n","  return \" \".join(noDuplicatePredictions)\n","\n","\n","\n","for i in range(1, 5):\n","  target_sequence = input_sequences[i].reshape(1, 75, 1)\n","  prediction = network.predict(target_sequence)\n","  output = outputToText(prediction[0], output_tokenizer)\n","  print(\"Input:\", wikipedia_sentences[i])\n","  print(\"Expected:\", simple_wikipedia_sentences[i])\n","  print(\"Result:\", output)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: ['Royal', 'Rumble', 'was', 'the', 'twentieth', 'annual', 'Royal', 'Rumble', 'professional', 'wrestling', 'pay-per-view', 'event', 'produced', 'by', 'World', 'Wrestling', 'Entertainment', '.']\n","Expected: ['Royal', 'Rumble', 'was', 'the', 'twentieth', 'yearly', 'Royal', 'Rumble', 'professional', 'wrestling', 'pay-per-view', 'event', 'made', 'by', 'World', 'Wrestling', 'Entertainment', '.']\n","Result: the rumble wrestling pay-per-view the wrestling . \n","Input: ['Janko', 'Prunk', 'is', 'a', 'Slovenian', 'historian', 'of', 'modern', 'history', '.']\n","Expected: ['Janko', 'Prunk', ',', 'is', 'a', 'Slovenian', 'historian', 'of', 'modern', 'history', '.']\n","Result: janko prunk is a slovenian historian of history . \n","Input: ['Sometimes', 'the', 'balalaika', 'is', 'tuned', '``', 'guitar', 'style', \"''\", 'to', 'G-B-D', ',', 'making', 'it', 'easier', 'to', 'play', 'for', 'Russian', 'guitar', 'players', ',', 'although', 'balalaika', 'purists', 'frown', 'on', 'this', 'tuning', '.']\n","Expected: ['The', 'Balalaika', 'family', 'includes', 'the', 'prima', 'balalaika', ',', 'sekunda', 'balalaika', ',', 'alto', 'balalaika', ',', 'bass', 'balalaika', 'and', 'contrabass', 'balalaika', '.', 'They', 'all', 'have', 'three', 'strings', 'and', 'two', 'of', 'them', 'are', 'tuned', 'to', 'the', 'same', 'note', '.']\n","Result: the balalaika sekunda balalaika , the . \n","Input: ['Two', 'days', 'after', 'he', 'was', 'sworn', 'in', 'as', 'Prime', 'Minister', ',', 'the', 'Wall', 'Street', 'Crash', 'of', '1929', 'occurred', ',', 'marking', 'the', 'beginning', 'of', 'the', 'Great', 'Depression', 'and', 'subsequent', 'Great', 'Depression', 'in', 'Australia', '.']\n","Expected: ['He', 'became', 'Prime', 'Minister', 'two', 'days', 'before', 'the', 'Great', 'Depression', 'began', '.']\n","Result: the was the in . \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kATzTNG6-jE_"},"source":["import pickle\n","\n","with open('drive/MyDrive/input_tokenizer5.pickle', 'wb') as handle:\n","    pickle.dump(input_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('drive/MyDrive/output_tokenizer5.pickle', 'wb') as handle:\n","    pickle.dump(output_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","model_json = network.to_json()\n","with open(\"drive/MyDrive/network5.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","    json_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gza_K2n7nPY_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618515538210,"user_tz":-60,"elapsed":1030,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"e591d22c-7cd5-4bf6-bb81-45ebb6e44f9e"},"source":["text = \"Royal Rumble was the twentieth annual Royal Rumble professional wrestling pay-per-view event produced by World Wrestling\"\n","\n","sequence = pad(input_tokenizer.texts_to_sequences([text]), 75)[0]\n","sequence = sequence.reshape(1, 75, 1)\n","#print(sequence)\n","\n","prediction = network.predict(sequence)\n","output = outputToText(prediction[0], output_tokenizer)\n","print(output)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the rumble the . \n"],"name":"stdout"}]}]}